{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from googlesearch import search\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Selenium Chrome Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Users/liamparker/Downloads/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "kaggle_csv = '/Users/liamparker/Downloads/train.csv'\n",
    "movie_df = pd.read_csv(kaggle_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'belongs_to_collection', 'budget', 'genres', 'homepage',\n",
       "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
       "       'popularity', 'poster_path', 'production_companies',\n",
       "       'production_countries', 'release_date', 'runtime', 'spoken_languages',\n",
       "       'status', 'tagline', 'title', 'Keywords', 'cast', 'crew', 'revenue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the columns in the Dataset\n",
    "movie_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform initial pruning\n",
    "\n",
    "# Remove non-english films\n",
    "en_df = movie_df.loc[movie_df['original_language'] == 'en']\n",
    "\n",
    "# Remove 0-budget films\n",
    "en_df = en_df.loc[en_df['budget'] > 0]\n",
    "\n",
    "# Remove films outside of 2010-2020\n",
    "en_df['release_date'] = en_df['release_date'].apply(lambda x: datetime.strptime(x, '%m/%d/%y'))\n",
    "en_df = en_df.loc[en_df['release_date'] > datetime(2010, 1, 1)]\n",
    "en_df = en_df.loc[en_df['release_date'] < datetime(2020, 1, 1)]\n",
    "\n",
    "# Drop unneeded columns\n",
    "en_df = en_df.drop(columns = ['popularity', 'production_companies', 'id', 'belongs_to_collection', \n",
    "                              'production_countries', 'spoken_languages', 'status', 'original_language', \n",
    "                              'poster_path', 'title', 'tagline', 'cast', 'crew'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all trailer links using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 50\n",
      "Iteration: 100\n",
      "Iteration: 150\n",
      "Iteration: 200\n",
      "Iteration: 250\n",
      "Iteration: 300\n",
      "Iteration: 350\n",
      "Iteration: 400\n",
      "Iteration: 450\n",
      "Iteration: 500\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "counter = 0\n",
    "\n",
    "for title in en_df['original_title']:\n",
    "    query = title + str(' Trailer')\n",
    "    results.append(next(search(query, tld=\"co.in\", num=1, stop=1, pause=2)))\n",
    "    if counter % 50 == 0:\n",
    "        print('Iteration:', counter)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join links to respective films in the dataset\n",
    "links_df = pd.DataFrame(results, columns=['link'])\n",
    "en_df = en_df.reset_index()\n",
    "en_df = en_df.join(links_df, how='outer')\n",
    "en_df = en_df.drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liamparker/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/liamparker/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# This code is now obsolete\n",
    "for i in range(len(en_df['homepage'])):\n",
    "    if isinstance(en_df['homepage'][i], str):\n",
    "        en_df['homepage'][i] = 1\n",
    "    else:\n",
    "        en_df['homepage'][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all films that do not have \"youtube\" in the link for trailer\n",
    "for i in range(len(en_df)):\n",
    "    if not 'youtube' in en_df['link'][i]:\n",
    "        en_df = en_df.drop(i)\n",
    "en_df = en_df.reset_index()\n",
    "len(en_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape data from Youtube using the Selenium Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(link, num_iters):\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    # Specify Driver\n",
    "    driver = Chrome(executable_path='/Users/liamparker/Downloads/chromedriver') \n",
    "    \n",
    "    # Open Link\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Retrieve every item in the link with TAG_NAME = \"body\" in order to scroll through the page \n",
    "    for item in range(num_iters): \n",
    "        vis = EC.visibility_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        wait.until(vis).send_keys(Keys.END)\n",
    "        time.sleep(5)\n",
    "        \n",
    "    # Retrieve all comments by searching for CSS_SELECTOR = \"#content\"\n",
    "    for comment in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#content\"))):\n",
    "        txt = comment.text\n",
    "        data.append(txt)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a list of lists of comments using the above definition\n",
    "full_comments = []\n",
    "for i in range(1):\n",
    "    try:\n",
    "        full_comments.append(scrape_data(en_df['link'][i], 10))\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to save comments list\n",
    "comments_dict = {}\n",
    "\n",
    "for i in range(len(full_comments)):\n",
    "    comments_dict[en_df['original_title'][i]] = full_comments[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the dictionary\n",
    "with open('saved_dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(comments_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the dictionary\n",
    "with open('saved_dictionary.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Number of Views and Likes for Each Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = []\n",
    "likes = []\n",
    "\n",
    "# Views and Likes Counts are found in the first Comment Returned from Each List\n",
    "for i in range(len(full_comments)):\n",
    "    c = full_comments[i][0]\n",
    "    views.append(re.split(r\"\\n(.*) views\", c)[1])\n",
    "    try:\n",
    "        likes.append(re.split(r\"\\n(.*)\\nDISLIKE\", c)[1])\n",
    "    except:\n",
    "        likes.append(float(\"nan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate index column created above\n",
    "en_df = en_df.drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append views and likes columns to the dataframe\n",
    "views_df = pd.DataFrame(views, columns=['views'])\n",
    "likes_df = pd.DataFrame(likes, columns=['likes'])\n",
    "en_df = en_df.join(views_df, how='outer')\n",
    "en_df = en_df.join(likes_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splice strings returning numbers and converting K to 1,000 and M to 1,000,000\n",
    "def value_to_float(x):\n",
    "    if type(x) == float or type(x) == int:\n",
    "        return x\n",
    "    if 'K' in x:\n",
    "        if len(x) > 1:\n",
    "            return float(x.replace('K', '')) * 1000\n",
    "        return 1000.0\n",
    "    if 'M' in x:\n",
    "        if len(x) > 1:\n",
    "            return float(x.replace('M', '')) * 1000000\n",
    "        return 1000000.0\n",
    "    else:\n",
    "        return float(x.replace(',', ''))\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liamparker/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/liamparker/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/liamparker/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/liamparker/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Perform the above operation\n",
    "for i in range(len(en_df)):\n",
    "    try:\n",
    "        en_df['likes'][i] = value_to_float(en_df['likes'][i])\n",
    "        en_df['views'][i] = value_to_float(en_df['views'][i])\n",
    "    except:\n",
    "        en_df['likes'][i] = float('nan')\n",
    "        en_df['views'][i] = float('nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Sentiment Analysis on the Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Tokenizer and Lemmatizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to perform VADER sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def sentiment_scores(sentence):\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sentiment.polarity_scores(sentence)\n",
    "    return np.array([sentiment_dict['neg'], sentiment_dict['neu'], sentiment_dict['pos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Sentiments for Every Movie\n",
    "negatives = []\n",
    "neutrals = []\n",
    "positives = []\n",
    "positives_perc = []\n",
    "negatives_perc = []\n",
    "\n",
    "# Iterate through all comments\n",
    "for j in range(len(full_comments)):\n",
    "    if j % 50 == 0:\n",
    "        # iterative update printing to track progress\n",
    "        print(j)\n",
    "    docs = loaded_dict[en_df['original_title'][j]][2:]\n",
    "    \n",
    "    # Append 0 if the comment list is empty\n",
    "    if not docs:\n",
    "        negatives.append(0)\n",
    "        neutrals.append(0)\n",
    "        positives.append(0)\n",
    "        continue\n",
    "    \n",
    "    # Tokenize all of the items in the doc\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = tokenizer.tokenize(docs[i])\n",
    "    \n",
    "    # Perform lemmatization, and remove stopwords, single character words, numbers\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    docs = [[token for token in doc if not token in stop_words] for doc in docs]\n",
    "    \n",
    "    # Join tokens back together to be fed into VADER sentiment analysis\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = ' '.join(docs[i])\n",
    "    \n",
    "    neg = 0\n",
    "    neu = 0\n",
    "    pos = 0\n",
    "    neg_perc = 0\n",
    "    pos_perc = 0\n",
    "    \n",
    "    # Perform VADER sentiment analysis using the function built above\n",
    "    for i in range(len(docs)):\n",
    "        comp_neg, comp_neu, comp_pos = sentiment_scores(docs[i])\n",
    "        neg += comp_neg\n",
    "        neu += comp_neu\n",
    "        pos += comp_pos\n",
    "    \n",
    "    # Append scores to lists\n",
    "    negatives.append(neg/len(docs))\n",
    "    neutrals.append(neu/len(docs))\n",
    "    positives.append(pos/len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join lists to the dataframe\n",
    "neg_df = pd.DataFrame(negatives, columns=['negative_sentiment'])\n",
    "neu_df = pd.DataFrame(neutrals, columns=['neutral_sentiment'])\n",
    "pos_df = pd.DataFrame(positives, columns=['positive_sentiment'])\n",
    "pos_perc_df = pd.DataFrame(positives_perc, columns=['positive_percentage'])\n",
    "neg_perc_df = pd.DataFrame(negatives_perc, columns=['negative_percentage'])\n",
    "en_df = en_df.join(neg_df, how='outer')\n",
    "en_df = en_df.join(neu_df, how='outer')\n",
    "en_df = en_df.join(pos_df, how='outer')\n",
    "en_df = en_df.join(pos_perc_df, how='outer')\n",
    "en_df = en_df.join(neg_perc_df, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the DataFrame to be Used in Colab (see Model Construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df['target'] = en_df['revenue']\n",
    "en_df = en_df.drop(columns = 'revenue')\n",
    "en_df = en_df.drop(columns = 'negative_percentage')\n",
    "en_df.to_excel('updated_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
